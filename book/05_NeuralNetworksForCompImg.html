
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural Networks for Computational Imaging &#8212; Computational Imaging - Course Notes</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Inverse Problems in Computational Imaging" href="06_InverseProblemsInCompimg.html" />
    <link rel="prev" title="Light Transport Analysis" href="04_LightTransportAnalysis.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Computational Imaging - Course Notes</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Computational Imaging - Course Notes for Winter Term 2022 / 2023
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="01_Introduction.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="02_Basics.html">
   Fundamental Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="03_LightFieldMethods.html">
   Light Field Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="04_LightTransportAnalysis.html">
   Light Transport Analysis
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Networks for Computational Imaging
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="06_InverseProblemsInCompimg.html">
   Inverse Problems in Computational Imaging
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/05_NeuralNetworksForCompImg.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download notebook file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-code"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        <a href="_sources/05_NeuralNetworksForCompImg.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#content">
   Content
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-neural-networks">
   Introduction to neural networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-building-blocks">
   Principal building blocks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-layers">
     Linear layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear-layers">
     Non-linear layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem">
   Universal approximation theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-of-network-parameters">
   Optimization of network parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-chain-rule-and-the-backpropagation-algorithm">
     The chain rule and the backpropagation algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-implementation-for-y-log-x-2">
       Example implementation for
       <span class="math notranslate nohighlight">
        \(y = \log (x)^2\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automatic-differentiation">
     Automatic differentiation
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural Networks for Computational Imaging</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#content">
   Content
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction-to-neural-networks">
   Introduction to neural networks
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principal-building-blocks">
   Principal building blocks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#layers">
     Layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-layers">
     Linear layers
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#non-linear-layers">
     Non-linear layers
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#universal-approximation-theorem">
   Universal approximation theorem
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#optimization-of-network-parameters">
   Optimization of network parameters
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gradient-descent">
     Gradient descent
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-chain-rule-and-the-backpropagation-algorithm">
     The chain rule and the backpropagation algorithm
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#example-implementation-for-y-log-x-2">
       Example implementation for
       <span class="math notranslate nohighlight">
        \(y = \log (x)^2\)
       </span>
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#automatic-differentiation">
     Automatic differentiation
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <div class="cell tag_remove-input tag_book_only docutils container">
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">interact</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="n">showFig</span><span class="p">(</span><span class="s1">&#39;figures/4/desk_lightsources_example_&#39;</span><span class="p">,</span><span class="n">i</span><span class="p">,</span><span class="s1">&#39;.svg&#39;</span><span class="p">,</span><span class="mi">800</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span> <span class="n">i</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">IntSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="p">(</span><span class="n">min_i</span><span class="o">:=</span><span class="mi">1</span><span class="p">),</span><span class="nb">max</span><span class="o">=</span><span class="p">(</span><span class="n">max_i</span><span class="o">:=</span><span class="mi">5</span><span class="p">),</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="p">(</span><span class="n">max_i</span> <span class="k">if</span> <span class="n">book</span> <span class="k">else</span> <span class="n">min_i</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "dcfb8042a91e4e20b6d8a429a9da3191"}
</script><div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;function __main__.&lt;lambda&gt;(i)&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">img</span> <span class="n">src</span><span class="o">=</span><span class="s2">&quot;figures/4/light_path_categories.png&quot;</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;max-width:30vw&quot;</span><span class="o">&gt;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">  File</span><span class="nn"> &quot;&lt;ipython-input-9-1642348dfef1&gt;&quot;</span><span class="gt">, line </span><span class="mi">1</span>
    <span class="o">&lt;</span><span class="n">img</span> <span class="n">src</span><span class="o">=</span><span class="s2">&quot;figures/4/light_path_categories.png&quot;</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;max-width:30vw&quot;</span><span class="o">&gt;</span>
    <span class="o">^</span>
<span class="ne">SyntaxError</span>: invalid syntax
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(\begin{align}
  \newcommand{transp}{^\intercal}
  \newcommand{F}{\mathcal{F}}
  \newcommand{Fi}{\mathcal{F}^{-1}}
  \newcommand{inv}{^{-1}}
  \newcommand{stochvec}[1]{\mathbf{\tilde{#1}}}
  \newcommand{argmax}[1]{\underset{#1}{\mathrm{arg\, max}}}
  \newcommand{argmin}[1]{\underset{#1}{\mathrm{arg\, min}}}
\end{align}\)</span></p>
<p><font size="7"> Computational Imaging </font><br><br><br></p>
<section class="tex2jax_ignore mathjax_ignore" id="neural-networks-for-computational-imaging">
<h1>Neural Networks for Computational Imaging<a class="headerlink" href="#neural-networks-for-computational-imaging" title="Permalink to this headline">#</a></h1>
<section id="content">
<h2>Content<a class="headerlink" href="#content" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Introduction to neural networks</p></li>
<li><p>Principal building blocks of a neural network</p></li>
<li><p>Universal approximation theorem</p></li>
<li><p>Gradient descent</p></li>
<li><p>Automatic differentiation</p></li>
<li><p>Architectures and loss functions</p></li>
<li><p>Regularization</p></li>
<li><p>Libraries, tools and other resources</p></li>
</ul>
</section>
<section id="introduction-to-neural-networks">
<h2>Introduction to neural networks<a class="headerlink" href="#introduction-to-neural-networks" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>(Artificial) neural networks represent a class of machine learning methods.</p></li>
<li><p>They can be imagined as versatile approximators <span class="math notranslate nohighlight">\(\varphi_\boldsymbol{\theta}\)</span> of arbitrary, continuous functions <span class="math notranslate nohighlight">\(\mathbf{y} = f(\mathbf{x}), \quad f:\mathbb{R}^N \mapsto \mathbb{R}^M,\quad M,N \in \mathbb{N}\)</span>, i.e., with <span class="math notranslate nohighlight">\(\varphi_\boldsymbol{\theta}(\mathbf{x}) \approx f(\mathbf{x})\)</span>.</p></li>
<li><p>They are defined by their architecture and the corresponding parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p></li>
</ul>
<ul class="simple">
<li><p>Via a suitable training procedure and a so-called <em>training set</em> of example pairs <span class="math notranslate nohighlight">\(\mathcal{T} = \left\{ (\mathbf{x}_i, \mathbf{y}_i), i \in \left[ 1,\ldots , N \right]  \right\}\)</span> of input variables <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> and corresponding output variables <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>, their parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> are optimized so that</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\forall (\mathbf{x},\mathbf{y}) \in \mathcal{T}:\text{dist}(\varphi_\boldsymbol{\theta}(\mathbf{x}), \mathbf{y} ) \rightarrow \text{Min.}\,,\)</span> with a suitable distance function <span class="math notranslate nohighlight">\(\text{dist}\)</span> and</p></li>
<li><p>(hopefully) <span class="math notranslate nohighlight">\(\text{dist}(\varphi_\boldsymbol{\theta}(\mathbf{x}), f(\mathbf{x}) ) \rightarrow \text{Min.}\,,\)</span> for unseen input vectors <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, i.e., which are not part of the training set.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>Neural networks have first been described in 1943 by Warren McCulloch and Walter Pitts in their paper “A Logical Calculus of the Ideas Immanent in Nervous Activity”.</p></li>
<li><p>Frank Rosenblatt followed their approach and described the so-called <em>Perceptron</em> as a fundamental unit of early neural networks.</p></li>
</ul>
<ul class="simple">
<li><p>Approximately around the year 2010, researchers started to use very deep neural networks, i.e., with many so-called <em>layers</em> (more information later) and achieved unprecedented performances on various tasks in the field of machine learning and computer vision.</p></li>
</ul>
<ul class="simple">
<li><p>An important enabler for this breakthrough were the increase in computing power provided by modern computers, especially by GPUs (graphics processing units), and the availability and usage of huge amounts of training data.</p></li>
</ul>
</section>
<section id="principal-building-blocks">
<h2>Principal building blocks<a class="headerlink" href="#principal-building-blocks" title="Permalink to this headline">#</a></h2>
<p>The two fundamental building blocks of neural networks are</p>
<ul class="simple">
<li><p>Matrix vector multiplications and</p></li>
<li><p>non-linear functions, also called or <em>activation functions</em>.</p></li>
</ul>
<p>Multiple instances of these building blocks can be stacked in parallel or consecutively with respect to each other to finally yield a neural network.</p>
<section id="layers">
<h3>Layers<a class="headerlink" href="#layers" title="Permalink to this headline">#</a></h3>
<p>When stacking linear or non-linear building blocks in parallel, the resulting structure is called a linear, respectively, a non-linear layer (usually either a linear or a non-linear block is stacked in parallel, not a mixture of both).</p>
<p>The way of stacking the individual blocks is called the <em>architecture</em> of the neural network.</p>
</section>
<section id="linear-layers">
<h3>Linear layers<a class="headerlink" href="#linear-layers" title="Permalink to this headline">#</a></h3>
<p>In one building block of a linear layer, the input <span class="math notranslate nohighlight">\((x_1, x_2, \ldots, x_N)\transp\)</span> is mapped to the scalar output <span class="math notranslate nohighlight">\(y\)</span> via a linear transformation, i.e.,</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   y = \sum\limits^{N}_{i=1} w_i \cdot x_i + b \,,
\end{align}\)</span></p>
<p>with <span class="math notranslate nohighlight">\(w_i\)</span> denoting the <span class="math notranslate nohighlight">\(i\)</span>-th so-called <em>weight</em>, i.e. parameter, of this block and <span class="math notranslate nohighlight">\(b\)</span> denoting the so-called <em>bias</em> (also a parameter), i.e., an additive term not depending on the input.</p>
<p>When one input is simultaneously processed by <span class="math notranslate nohighlight">\(K\)</span> linear blocks, i.e.,</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   y_k = \sum\limits^{N}_{i=1} w^k_i \cdot x_i + b^k = \underbrace{\left( w^k_1, w^k_2, \ldots, w^k_N, b^k \right)}_{\mathbf{w}\transp_k} \cdot \underbrace{\begin{pmatrix} 
      x_1 \\ x_2 \\ \vdots \\ x_N \\ 1
   \end{pmatrix}}_{\mathbf{x}}  \,,
\end{align}\)</span></p>
<p>for block <span class="math notranslate nohighlight">\(k\)</span>, this can be expressed compactly via matrix-vector multiplications:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   \begin{pmatrix} 
      y_1 \\ y_2 \\ \vdots \\ y_K
   \end{pmatrix}
   &amp;= 
   \begin{pmatrix} 
      \qquad \mathbf{w}\transp_1 \qquad  \\ \mathbf{w}\transp_2 \\ \vdots \\ \mathbf{w}\transp_K
   \end{pmatrix} \cdot 
   \begin{pmatrix} 
      x_1 \\ x_2 \\ \vdots \\ x_N \\ 1
   \end{pmatrix} \\
   &amp;= \qquad \quad \mathbf{W} \quad \qquad \cdot \quad  \mathbf{x} \,,
\end{align}\)</span></p>
<p>with <span class="math notranslate nohighlight">\(\mathbf{W} \in \mathbb{R}^{K \times (N+1)}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{(N+1) \times 1}\)</span>.</p>
<p>This expression can be further extended for the case when multiple input vectors <span class="math notranslate nohighlight">\(\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_B\)</span>, i.e., a so-called <em>batch</em> of size <span class="math notranslate nohighlight">\(B\)</span>, have to be processed simultaneously:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
  \begin{pmatrix} 
    \qquad \mathbf{w}\transp_1 \qquad  \\ \mathbf{w}\transp_2 \\ \vdots \\ \mathbf{w}\transp_K
 \end{pmatrix} \cdot 
 \begin{pmatrix} 
     \\  \\ \mathbf{x}_1 &amp; \mathbf{x}_2 &amp; \cdots &amp; \mathbf{x}_B    \\ \\ \\
 \end{pmatrix} &amp;= \begin{pmatrix} 
    \mathbf{w}\transp_1 \cdot \mathbf{x}_1 &amp; \mathbf{w}\transp_1 \cdot \mathbf{x}_2 &amp;\cdots &amp;\mathbf{w}\transp_1 \cdot \mathbf{x}_B \\ 
    \mathbf{w}\transp_2 \cdot \mathbf{x}_1 &amp; \mathbf{w}\transp_2 \cdot \mathbf{x}_2 &amp;\cdots &amp;\mathbf{w}\transp_2 \cdot \mathbf{x}_B \\     
    \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
    \mathbf{w}\transp_K \cdot \mathbf{x}_1 &amp; \mathbf{w}\transp_K \cdot \mathbf{x}_2 &amp;\cdots &amp;\mathbf{w}\transp_K \cdot \mathbf{x}_B \\ 
 \end{pmatrix}\\
 &amp;= \begin{pmatrix} 
  \\  \\ \mathbf{y}_1 &amp; \mathbf{y}_2 &amp; \cdots &amp; \mathbf{y}_B    \\ \\ \\
\end{pmatrix}
 \,.
\end{align}\)</span></p>
</section>
<section id="non-linear-layers">
<h3>Non-linear layers<a class="headerlink" href="#non-linear-layers" title="Permalink to this headline">#</a></h3>
<p>Neural networks constructed only out of linear layers are very limited in their approximation abilities since in essence they just represent a long linear function and hence can only mimic linear functions.</p>
<p>This is why additional, so-called <em>non-linear layers</em> consisting of non-linear building blocks are necessary.</p>
<p>In general, a non-linear building block is a non-linear function <span class="math notranslate nohighlight">\(\psi:\mathbb{R}\rightarrow \mathbb{R}\)</span> that is applied to the scalar output of a linear building block.</p>
<p>A popular example for a non-linearity is the so-called <em>sigmoid</em>-function</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
  \psi(y)=\frac{1}{1+\mathrm{e}^{-y} } \,.
\end{align}\)</span></p>
<p>When choosing high values for <span class="math notranslate nohighlight">\(\mathbf{W}\)</span>, the sigmoid-function resembles a unit step-function which can be shifted left or right by adjusting the bias <span class="math notranslate nohighlight">\(b\)</span>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="n">x</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">plot_sigmoid_after_linear</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">y1s</span> <span class="o">=</span> <span class="n">xs</span><span class="o">*</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">y2s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">y1s</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">y2s</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">interact</span><span class="p">(</span><span class="k">lambda</span> <span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">:</span> <span class="n">plot_sigmoid_after_linear</span><span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">),</span> <span class="n">w</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">8</span><span class="p">),</span> <span class="n">b</span><span class="o">=</span><span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>In batch processing, he result of a batch of data processed by a linear layer, i.e. <span class="math notranslate nohighlight">\(\mathbf{y} = \mathbf{Wx}\)</span>, is processed by the non-linearity in an element-wise fashion.</p>
</section>
</section>
<section id="universal-approximation-theorem">
<h2>Universal approximation theorem<a class="headerlink" href="#universal-approximation-theorem" title="Permalink to this headline">#</a></h2>
<p>It could be shown, that a neural network consisting only of one (sufficiently large) linear layer and one non-linear layer which are combined by a single linear building block can approximate any continuous function.</p>
<p>TODO: Add figure.</p>
<p>In the following, we will sketch the proof of that theorem.</p>
<p>Consider again the sigmoid function <span class="math notranslate nohighlight">\(\psi\)</span> from before applied to a linear block, i.e., <span class="math notranslate nohighlight">\(\psi(wx+b)\)</span>  form before.</p>
<p>The position of the unit step approximated by <span class="math notranslate nohighlight">\(\psi\)</span> for high <span class="math notranslate nohighlight">\(w\)</span> resides at the position <span class="math notranslate nohighlight">\(s=-\frac{b}{w}\)</span>. Since this is easier to interpret, we will focus on the parameter <span class="math notranslate nohighlight">\(s\)</span> from now on.</p>
<p>We now consider two of such blocks added together by an additional single linear building block, i.e.,</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   \psi_{s_1} (x)\cdot w_1 + \psi_{s_2}(x)\cdot w_2  + b
\end{align}\)</span></p>
<p>with the respective positions <span class="math notranslate nohighlight">\(s_1, s_2\)</span> of the step functions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="p">):</span>
    <span class="n">w</span> <span class="o">=</span> <span class="mi">1000</span>
    <span class="n">b</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">w</span> <span class="o">*</span> <span class="n">s</span>
    <span class="k">return</span> <span class="n">x</span><span class="o">*</span><span class="n">w</span> <span class="o">+</span> <span class="n">b</span>
<span class="k">def</span> <span class="nf">plot_2_neurons</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">):</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">y1s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">s1</span><span class="p">))</span>
    <span class="n">y2s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">s2</span><span class="p">))</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">y1s</span> <span class="o">+</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">y2s</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">res</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">interact</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">:</span> <span class="n">plot_2_neurons</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> \
         <span class="n">s1</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span> \
         <span class="n">s2</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.6</span><span class="p">),</span> \
         <span class="n">w1</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.4</span><span class="p">),</span> \
         <span class="n">w2</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.6</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>As can be seen, this addition yields to consecutive step functions what can be used, e.g., to approximate the <span class="math notranslate nohighlight">\(\mathrm{rect}\)</span>-function.</p>
<p>Therefore, if <span class="math notranslate nohighlight">\(s_1 &lt; s_2\)</span>, it must hold <span class="math notranslate nohighlight">\(w_2 = -w_1\)</span> to get a <span class="math notranslate nohighlight">\(\mathrm{rect}\)</span>-function with height <span class="math notranslate nohighlight">\(h=\left| w_1 \right| = \left| w_2 \right|  \)</span>.</p>
<p>We can now add two of such pairs of blocks together to model two <span class="math notranslate nohighlight">\(\mathrm{rect}\)</span>-functions, i.e., with start, stop positions <span class="math notranslate nohighlight">\(s_{1,1}, s_{1,2}\)</span> and height <span class="math notranslate nohighlight">\(h_1\)</span> of the first <span class="math notranslate nohighlight">\(\mathrm{rect}\)</span>-function and <span class="math notranslate nohighlight">\(s_{2,1}, s_{2,2}, h_2\)</span> for the second one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rect_approx</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span> <span class="n">h</span><span class="p">):</span>
    <span class="n">w1</span> <span class="o">=</span> <span class="n">h</span>
    <span class="n">w2</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">h</span>
    <span class="n">y1s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s1</span><span class="p">))</span>
    <span class="n">y2s</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">w1</span> <span class="o">*</span> <span class="n">y1s</span> <span class="o">+</span> <span class="n">w2</span> <span class="o">*</span> <span class="n">y2s</span>
    
<span class="k">def</span> <span class="nf">plot_2_rects</span><span class="p">(</span><span class="n">s11</span><span class="p">,</span> <span class="n">s12</span><span class="p">,</span> <span class="n">s21</span><span class="p">,</span> <span class="n">s22</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">):</span>
    <span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
    <span class="n">y1s</span> <span class="o">=</span> <span class="n">rect_approx</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">s11</span><span class="p">,</span> <span class="n">s12</span><span class="p">,</span> <span class="n">h1</span><span class="p">)</span>
    <span class="n">y2s</span> <span class="o">=</span> <span class="n">rect_approx</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">s21</span><span class="p">,</span> <span class="n">s22</span><span class="p">,</span> <span class="n">h2</span><span class="p">)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">y1s</span> <span class="o">+</span> <span class="n">y2s</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">clf</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">res</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">interact</span><span class="p">(</span><span class="k">lambda</span> <span class="n">s11</span><span class="p">,</span> <span class="n">s12</span><span class="p">,</span> <span class="n">s21</span><span class="p">,</span> <span class="n">s22</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">:</span> <span class="n">plot_2_rects</span><span class="p">(</span><span class="n">s11</span><span class="p">,</span> <span class="n">s12</span><span class="p">,</span> <span class="n">s21</span><span class="p">,</span> <span class="n">s22</span><span class="p">,</span> <span class="n">h1</span><span class="p">,</span> <span class="n">h2</span><span class="p">),</span> \
         <span class="n">s11</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span> \
         <span class="n">s12</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span> \
         <span class="n">s21</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.4</span><span class="p">),</span> \
         <span class="n">s22</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.6</span><span class="p">),</span> \
         <span class="n">h1</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span> \
         <span class="n">h2</span> <span class="o">=</span> <span class="n">widgets</span><span class="o">.</span><span class="n">FloatSlider</span><span class="p">(</span><span class="nb">min</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span><span class="nb">max</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mf">0.4</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>The more of these modules we add, the more complicated the shape of the output can be. When the width of the single <span class="math notranslate nohighlight">\(\mathrm{rect}\)</span>-functions approaches zero and the number of <span class="math notranslate nohighlight">\(\mathrm{rect}\)</span>-functions approaches infinity, any continuous function can be approximated.</p>
<p>This also holds for higher dimensions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">0.2</span><span class="p">,</span><span class="mf">1.2</span><span class="p">,</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">s1s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">])</span>
<span class="n">s2s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span>
<span class="n">hs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.8</span><span class="p">])</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">s1s</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="o">+</span> <span class="n">rect_approx</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">s1s</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">s2s</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">hs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Although the theorem states that such a simple structure is enough to approximate any function, in practice network structures with more but thinner layers (i.e., with fewer building blocks) are employed as they can achieve similar results with notably fewer building blocks.</p>
</section>
<section id="optimization-of-network-parameters">
<h2>Optimization of network parameters<a class="headerlink" href="#optimization-of-network-parameters" title="Permalink to this headline">#</a></h2>
<p>How can we automatically determine the parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> of the network so that <span class="math notranslate nohighlight">\(\forall (\mathbf{x},\mathbf{y}) \in \mathcal{T}:\text{dist}(\varphi_\boldsymbol{\theta}(\mathbf{x}), \mathbf{y} ) \rightarrow \text{Min.}\,\)</span>?</p>
<section id="gradient-descent">
<h3>Gradient descent<a class="headerlink" href="#gradient-descent" title="Permalink to this headline">#</a></h3>
<p>To find this minimum, we compute the gradient <span class="math notranslate nohighlight">\(\nabla\text{dist}_\boldsymbol{\theta}\)</span> of the distance function <span class="math notranslate nohighlight">\(\text{dist}\)</span> with respect to the network parameters <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>.</p>
<p>We can then iteratively update an initial guess <span class="math notranslate nohighlight">\(\hat{\boldsymbol{\theta}}_0\)</span> (e.g., random) of the network parameters by pushing it into the inverse direction of the gradient <span class="math notranslate nohighlight">\(\nabla\text{dist}_\boldsymbol{\theta}\)</span>, i.e., into the direction of the nearest minimum:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   \hat{\boldsymbol{\theta}}_{i+1} \leftarrow \hat{\boldsymbol{\theta}}_{i} - \eta \nabla\text{dist}_\boldsymbol{\theta}(\varphi_{\hat{\boldsymbol{\theta}_i}}(\mathbf{X}), \mathbf{Y} )\,, (\mathbf{X},\mathbf{Y}) \in \mathcal{T}
\end{align}\)</span></p>
<p>with <span class="math notranslate nohighlight">\(\mathbf{X}, \mathbf{Y}\)</span> indicating batches of multiple training vectors <span class="math notranslate nohighlight">\((\mathbf{x}, \mathbf{y}) \in \mathcal{T}\)</span> and <span class="math notranslate nohighlight">\(\eta\)</span> denoting the so-called <em>learning rate</em> or the <em>step size</em> for the gradient descent updates.</p>
<ul class="simple">
<li><p>In every gradient descent iteration, the parameters are updated with regard to the respective batch of training samples <span class="math notranslate nohighlight">\((\mathbf{X}, \mathbf{Y})\)</span> chosen in that iteration.</p></li>
<li><p>In practical scenarios it is usually not possible to process the whole training set in one gradient descent step as the respective data would not fit into the available memory.</p></li>
<li><p>Hence, in every iteration another batch of training data is used, so that eventually all training samples have been used. The partitioning of <span class="math notranslate nohighlight">\(\mathcal{T}\)</span> into those batches is usually performed randomly. This is why this kind of gradient descent is sometimes referred to as <em>stochastic gradient descent</em>.</p></li>
<li><p>The set of gradient descent iterations needed to cycle through all training data once is a so-called <em>epoch</em>.</p></li>
</ul>
</section>
<section id="the-chain-rule-and-the-backpropagation-algorithm">
<h3>The chain rule and the backpropagation algorithm<a class="headerlink" href="#the-chain-rule-and-the-backpropagation-algorithm" title="Permalink to this headline">#</a></h3>
<p>Calculating the gradient required for gradient descent by hand can be infeasible for large expressions. Fortunately, this process can be automated as we will see in the following.</p>
<p>We start with an example.</p>
<p>Assume we want to calculate the derivative (i.e., the gradient) of <span class="math notranslate nohighlight">\(y = \log (x)^2\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p>We can express this function via intermediate variables for all basic math operations:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   y &amp;= v_2 \\
   v_2 &amp;= v_1^2 \\
   v_1 &amp;= \log (x) \,.
\end{align}\)</span></p>
<p>In order to find the derivate of <span class="math notranslate nohighlight">\(y\)</span> with respect to <span class="math notranslate nohighlight">\(x\)</span>, we can iteratively apply the chain rule to obtain:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
   \frac{\partial y}{\partial x} = \frac{\partial y}{\partial v_2} \frac{\partial v_2}{\partial x} = \frac{\partial y}{\partial v_2} \frac{\partial v_2}{\partial v_1} \frac{\partial v_1}{\partial x} \,.
\end{align}\)</span></p>
<p>Those partial derivatives can easily be obtained for the respective terms:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
  \frac{\partial y}{\partial v_2} &amp;= 1 \\
  \frac{\partial v_2}{\partial v_1} &amp;= 2v_1 \\
  \frac{\partial v_1}{\partial x} &amp;= \frac{1}{x}.
\end{align}\)</span></p>
<p>Hence, the sought derivate of <span class="math notranslate nohighlight">\(y\)</span> w.r.t. <span class="math notranslate nohighlight">\(x\)</span> is given by:</p>
<p><span class="math notranslate nohighlight">\(\begin{align} 
  \frac{\partial y}{\partial x} = \frac{\partial y}{\partial v_2} \frac{\partial v_2}{\partial v_1} \frac{\partial v_1}{\partial x} = 1\cdot 2v_1 \cdot \frac{1}{x} \,.
\end{align}\)</span></p>
<p>This expression can be evaluated in reverse order (the so-called <em>backward pass</em>), i.e., from left to right, when the intermediate variables <span class="math notranslate nohighlight">\(v_2, v_1\)</span> have been evaluated for some <span class="math notranslate nohighlight">\(x\)</span> in the so-called <em>forward pass</em>.</p>
<p>When every basic mathematical function (e.g., <span class="math notranslate nohighlight">\(+, -, \exp, \log, \sin, \ldots\)</span>) also provides a method for the calculation of its gradient, the process of obtaining the derivative of an arbitrary complex expression, with respect to some variable and given the actual input numbers for the parameter(s), can be automated.</p>
<p>During the forward pass, a calculation graph is constructed that represents the hierarchical relations of the individual mathematical functions in terms of the chain rule. The intermediate results for every operation are saved in the nodes of this graph. Then, the graph can be traversed backwards starting from all leaves and intermediate gradient values can be calculated for every node and can then be backpropagated to the root yielding the sought gradient.</p>
<p>This approach is also known as the <em>backpropagation algorithm</em>.</p>
<section id="example-implementation-for-y-log-x-2">
<h4>Example implementation for <span class="math notranslate nohighlight">\(y = \log (x)^2\)</span><a class="headerlink" href="#example-implementation-for-y-log-x-2" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">square</span><span class="p">(</span><span class="n">inp</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">inp</span><span class="o">**</span><span class="mi">2</span>

<span class="k">def</span> <span class="nf">ln</span><span class="p">(</span><span class="n">inp</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">square_grad</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mi">2</span><span class="o">*</span><span class="n">inp</span><span class="o">*</span><span class="n">out</span><span class="o">.</span><span class="n">g</span>
    
<span class="k">def</span> <span class="nf">ln_grad</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">out</span><span class="p">):</span>
    <span class="n">inp</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">inp</span> <span class="o">*</span> <span class="n">out</span><span class="o">.</span><span class="n">g</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">v1</span> <span class="o">=</span> <span class="n">ln</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">v2</span> <span class="o">=</span> <span class="n">square</span><span class="p">(</span><span class="n">r1</span><span class="p">)</span>
        
    <span class="n">v2</span><span class="o">.</span><span class="n">g</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">square_grad</span><span class="p">(</span><span class="n">v1</span><span class="p">,</span> <span class="n">v2</span><span class="p">)</span>
    <span class="n">ln_grad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">v1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">v2</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">g</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note: We just used PyTorch here so that we can easily introduce the property <code class="docutils literal notranslate"><span class="pre">.g</span></code> for all our variables.</p>
</section>
</section>
<section id="automatic-differentiation">
<h3>Automatic differentiation<a class="headerlink" href="#automatic-differentiation" title="Permalink to this headline">#</a></h3>
<p>There are various libraries that provide the functionality of backpropagation mentioned above without the user (i.e., the programmer) having to explicitly control it. Often a technique called <em>operator overloading</em> is employed that builds the computation graph required for the gradient calculation behind the scenes.</p>
<p>In PyTorch, calculating the gradient of our equation <span class="math notranslate nohighlight">\(y = \log (x)^2\)</span> w.r.t. <span class="math notranslate nohighlight">\(x\)</span> could be achieved like this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Definition of our function using torch-routines:</span>
<span class="k">def</span> <span class="nf">y_torch</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">)</span>    <span class="c1"># Our input variable</span>
<span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="kc">True</span>   <span class="c1"># We need the gradient with respect to x</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">y_torch</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>      <span class="c1"># Forward pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>        <span class="c1"># Backward pass</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">grad</span>                   <span class="c1"># Access the gradient</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "compimg"
        },
        kernelOptions: {
            kernelName: "compimg",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'compimg'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="04_LightTransportAnalysis.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Light Transport Analysis</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="06_InverseProblemsInCompimg.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Inverse Problems in Computational Imaging</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Dr.-Ing. Johannes Meyer<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>