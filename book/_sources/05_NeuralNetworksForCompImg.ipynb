{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0a828b3",
   "metadata": {
    "init_cell": true,
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var cells = Jupyter.notebook.get_cells();\n",
       "           for (var i = 0; i < cells.length; i++) {\n",
       "               var cur_cell = cells[i];\n",
       "               var tags = cur_cell._metadata.tags;\n",
       "               console.log(i);\n",
       "               console.log(tags == undefined);\n",
       "               console.log(tags);\n",
       "               if (tags != undefined) {\n",
       "               for (var j = 0; j < tags.length; j++) {\n",
       "                  if (tags[j]==\"book_only\" | tags[j]==\"remove-cell\") {cur_cell.element.hide();}\n",
       "                  if (tags[j]==\"presentation_only\") {cur_cell.element.show();}\n",
       "            }}}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var cells = Jupyter.notebook.get_cells();\n",
    "           for (var i = 0; i < cells.length; i++) {\n",
    "               var cur_cell = cells[i];\n",
    "               var tags = cur_cell._metadata.tags;\n",
    "               console.log(i);\n",
    "               console.log(tags == undefined);\n",
    "               console.log(tags);\n",
    "               if (tags != undefined) {\n",
    "               for (var j = 0; j < tags.length; j++) {\n",
    "                  if (tags[j]==\"book_only\" | tags[j]==\"remove-cell\") {cur_cell.element.hide();}\n",
    "                  if (tags[j]==\"presentation_only\") {cur_cell.element.show();}\n",
    "            }}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c2bb04",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\meyjoh\\repos\\vlcompimg\\compimg\\lib\\site-packages\\scipy\\__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy.signal as sig\n",
    "from scipy.integrate import simpson\n",
    "from IPython.display import SVG, display, IFrame, HTML\n",
    "import seaborn as sns\n",
    "import torch\n",
    "%matplotlib notebook\n",
    "book = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64bbfa60",
   "metadata": {
    "tags": [
     "remove-input",
     "book_only"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "book = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bed3923",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def showFig2(path,i,ending, width, height):\n",
    "    imgToShow = plt.imread(f\"{path}{i}{ending}\")\n",
    "    plt.imshow(imgToShow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b748b26b",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def showFig(path,i,ending, width, height):\n",
    "    filename = path+str(i)+ending\n",
    "    return HTML(\"<img src=\\\"\" + filename +  f\"\\\" style=\\\"max-height:{height}vh\\\"/>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e938bec5",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def imshow(img, cmap=None):\n",
    "    plt.close('all')\n",
    "    plt.figure()\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da6b5ddb",
   "metadata": {
    "init_cell": true,
    "slideshow": {
     "slide_type": "notes"
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "def imshow2(img, cmap=None):\n",
    "    #plt.close('all')\n",
    "    #plt.figure()\n",
    "    plt.clf()\n",
    "    plt.imshow(img, cmap=cmap)\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d4b5183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfb8042a91e4e20b6d8a429a9da3191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='i', max=5, min=1), Output()), _dom_classes=('widget-inteâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(i)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(lambda i: showFig('figures/4/desk_lightsources_example_',i,'.svg',800,50), i=widgets.IntSlider(min=(min_i:=1),max=(max_i:=5), step=1, value=(max_i if book else min_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7fb3ecd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-1642348dfef1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-1642348dfef1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    <img src=\"figures/4/light_path_categories.png\" style=\"max-width:30vw\">\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "<img src=\"figures/4/light_path_categories.png\" style=\"max-width:30vw\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd25db",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$\\begin{align}\n",
    "  \\newcommand{transp}{^\\intercal}\n",
    "  \\newcommand{F}{\\mathcal{F}}\n",
    "  \\newcommand{Fi}{\\mathcal{F}^{-1}}\n",
    "  \\newcommand{inv}{^{-1}}\n",
    "  \\newcommand{stochvec}[1]{\\mathbf{\\tilde{#1}}}\n",
    "  \\newcommand{argmax}[1]{\\underset{#1}{\\mathrm{arg\\, max}}}\n",
    "  \\newcommand{argmin}[1]{\\underset{#1}{\\mathrm{arg\\, min}}}\n",
    "\\end{align}$\n",
    "\n",
    "<font size=\"7\"> Computational Imaging </font><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862ab12",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Neural Networks for Computational Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322a746",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Content\n",
    "* Introduction to neural networks\n",
    "* Principal building blocks of a neural network\n",
    "* Universal approximation theorem\n",
    "* Gradient descent\n",
    "* Automatic differentiation\n",
    "* Architectures and loss functions\n",
    "* Regularization\n",
    "* Libraries, tools and other resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ab8ea6",
   "metadata": {},
   "source": [
    "## Introduction to neural networks\n",
    "\n",
    "* (Artificial) neural networks represent a class of machine learning methods.\n",
    "* They can be imagined as versatile approximators $\\varphi_\\boldsymbol{\\theta}$ of arbitrary, continuous functions $\\mathbf{y} = f(\\mathbf{x}), \\quad f:\\mathbb{R}^N \\mapsto \\mathbb{R}^M,\\quad M,N \\in \\mathbb{N}$, i.e., with $\\varphi_\\boldsymbol{\\theta}(\\mathbf{x}) \\approx f(\\mathbf{x})$.\n",
    "* They are defined by their architecture and the corresponding parameters $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dfea13",
   "metadata": {},
   "source": [
    "* Via a suitable training procedure and a so-called *training set* of example pairs $\\mathcal{T} = \\left\\{ (\\mathbf{x}_i, \\mathbf{y}_i), i \\in \\left[ 1,\\ldots , N \\right]  \\right\\}$ of input variables $\\mathbf{x}_i$ and corresponding output variables $\\mathbf{y}_i$, their parameters $\\boldsymbol{\\theta}$ are optimized so that \n",
    "  * $\\forall (\\mathbf{x},\\mathbf{y}) \\in \\mathcal{T}:\\text{dist}(\\varphi_\\boldsymbol{\\theta}(\\mathbf{x}), \\mathbf{y} ) \\rightarrow \\text{Min.}\\,,$ with a suitable distance function $\\text{dist}$ and\n",
    "  * (hopefully) $\\text{dist}(\\varphi_\\boldsymbol{\\theta}(\\mathbf{x}), f(\\mathbf{x}) ) \\rightarrow \\text{Min.}\\,,$ for unseen input vectors $\\mathbf{x}$, i.e., which are not part of the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742c8f6",
   "metadata": {},
   "source": [
    "* Neural networks have first been described in 1943 by Warren McCulloch and Walter Pitts in their paper \"A Logical Calculus of the Ideas Immanent in Nervous Activity\".\n",
    "* Frank Rosenblatt followed their approach and described the so-called *Perceptron* as a fundamental unit of early neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b681459",
   "metadata": {},
   "source": [
    "* Approximately around the year 2010, researchers started to use very deep neural networks, i.e., with many so-called *layers* (more information later) and achieved unprecedented performances on various tasks in the field of machine learning and computer vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1cc32",
   "metadata": {},
   "source": [
    "* An important enabler for this breakthrough were the increase in computing power provided by modern computers, especially by GPUs (graphics processing units), and the availability and usage of huge amounts of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b0c582",
   "metadata": {},
   "source": [
    "## Principal building blocks\n",
    "\n",
    "The two fundamental building blocks of neural networks are\n",
    "* Matrix vector multiplications and\n",
    "* non-linear functions, also called or *activation functions*.\n",
    "\n",
    "Multiple instances of these building blocks can be stacked in parallel or consecutively with respect to each other to finally yield a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d053d4a4",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "When stacking linear or non-linear building blocks in parallel, the resulting structure is called a linear, respectively, a non-linear layer (usually either a linear or a non-linear block is stacked in parallel, not a mixture of both)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a08ed3",
   "metadata": {},
   "source": [
    "The way of stacking the individual blocks is called the *architecture* of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c2e8b6",
   "metadata": {},
   "source": [
    "### Linear layers\n",
    "\n",
    "In one building block of a linear layer, the input $(x_1, x_2, \\ldots, x_N)\\transp$ is mapped to the scalar output $y$ via a linear transformation, i.e.,\n",
    "\n",
    "$\\begin{align} \n",
    "   y = \\sum\\limits^{N}_{i=1} w_i \\cdot x_i + b \\,,\n",
    "\\end{align}$\n",
    "\n",
    "with $w_i$ denoting the $i$-th so-called *weight*, i.e. parameter, of this block and $b$ denoting the so-called *bias* (also a parameter), i.e., an additive term not depending on the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6415d1",
   "metadata": {},
   "source": [
    "When one input is simultaneously processed by $K$ linear blocks, i.e.,\n",
    "\n",
    "$\\begin{align} \n",
    "   y_k = \\sum\\limits^{N}_{i=1} w^k_i \\cdot x_i + b^k = \\underbrace{\\left( w^k_1, w^k_2, \\ldots, w^k_N, b^k \\right)}_{\\mathbf{w}\\transp_k} \\cdot \\underbrace{\\begin{pmatrix} \n",
    "      x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\\\ 1\n",
    "   \\end{pmatrix}}_{\\mathbf{x}}  \\,,\n",
    "\\end{align}$\n",
    "\n",
    "for block $k$, this can be expressed compactly via matrix-vector multiplications:\n",
    "\n",
    "$\\begin{align} \n",
    "   \\begin{pmatrix} \n",
    "      y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_K\n",
    "   \\end{pmatrix}\n",
    "   &= \n",
    "   \\begin{pmatrix} \n",
    "      \\qquad \\mathbf{w}\\transp_1 \\qquad  \\\\ \\mathbf{w}\\transp_2 \\\\ \\vdots \\\\ \\mathbf{w}\\transp_K\n",
    "   \\end{pmatrix} \\cdot \n",
    "   \\begin{pmatrix} \n",
    "      x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_N \\\\ 1\n",
    "   \\end{pmatrix} \\\\\n",
    "   &= \\qquad \\quad \\mathbf{W} \\quad \\qquad \\cdot \\quad  \\mathbf{x} \\,,\n",
    "\\end{align}$\n",
    "\n",
    "with $\\mathbf{W} \\in \\mathbb{R}^{K \\times (N+1)}$ and $\\mathbf{x} \\in \\mathbb{R}^{(N+1) \\times 1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe0310",
   "metadata": {},
   "source": [
    "This expression can be further extended for the case when multiple input vectors $\\mathbf{x}_1, \\mathbf{x}_2, \\ldots, \\mathbf{x}_B$, i.e., a so-called *batch* of size $B$, have to be processed simultaneously:\n",
    "\n",
    "$\\begin{align} \n",
    "  \\begin{pmatrix} \n",
    "    \\qquad \\mathbf{w}\\transp_1 \\qquad  \\\\ \\mathbf{w}\\transp_2 \\\\ \\vdots \\\\ \\mathbf{w}\\transp_K\n",
    " \\end{pmatrix} \\cdot \n",
    " \\begin{pmatrix} \n",
    "     \\\\  \\\\ \\mathbf{x}_1 & \\mathbf{x}_2 & \\cdots & \\mathbf{x}_B    \\\\ \\\\ \\\\\n",
    " \\end{pmatrix} &= \\begin{pmatrix} \n",
    "    \\mathbf{w}\\transp_1 \\cdot \\mathbf{x}_1 & \\mathbf{w}\\transp_1 \\cdot \\mathbf{x}_2 &\\cdots &\\mathbf{w}\\transp_1 \\cdot \\mathbf{x}_B \\\\ \n",
    "    \\mathbf{w}\\transp_2 \\cdot \\mathbf{x}_1 & \\mathbf{w}\\transp_2 \\cdot \\mathbf{x}_2 &\\cdots &\\mathbf{w}\\transp_2 \\cdot \\mathbf{x}_B \\\\     \n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\mathbf{w}\\transp_K \\cdot \\mathbf{x}_1 & \\mathbf{w}\\transp_K \\cdot \\mathbf{x}_2 &\\cdots &\\mathbf{w}\\transp_K \\cdot \\mathbf{x}_B \\\\ \n",
    " \\end{pmatrix}\\\\\n",
    " &= \\begin{pmatrix} \n",
    "  \\\\  \\\\ \\mathbf{y}_1 & \\mathbf{y}_2 & \\cdots & \\mathbf{y}_B    \\\\ \\\\ \\\\\n",
    "\\end{pmatrix}\n",
    " \\,.\n",
    "\\end{align}$\n",
    "\n",
    "### Non-linear layers\n",
    "\n",
    "Neural networks constructed only out of linear layers are very limited in their approximation abilities since in essence they just represent a long linear function and hence can only mimic linear functions.\n",
    "\n",
    "This is why additional, so-called *non-linear layers* consisting of non-linear building blocks are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c48c74",
   "metadata": {},
   "source": [
    "In general, a non-linear building block is a non-linear function $\\psi:\\mathbb{R}\\rightarrow \\mathbb{R}$ that is applied to the scalar output of a linear building block."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e2a3a",
   "metadata": {},
   "source": [
    "A popular example for a non-linearity is the so-called *sigmoid*-function \n",
    "\n",
    "$\\begin{align} \n",
    "  \\psi(y)=\\frac{1}{1+\\mathrm{e}^{-y} } \\,.\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32cfcef",
   "metadata": {},
   "source": [
    "When choosing high values for $\\mathbf{W}$, the sigmoid-function resembles a unit step-function which can be shifted left or right by adjusting the bias $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d24dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-1*x))\n",
    "def plot_sigmoid_after_linear(a,b):\n",
    "    xs = np.linspace(0,1,1000)\n",
    "    y1s = xs*a + b\n",
    "    y2s = sigmoid(y1s)\n",
    "    plt.clf()\n",
    "    plt.plot(xs,y2s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e94ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "interact(lambda w,b: plot_sigmoid_after_linear(w,b), w=widgets.FloatSlider(min=1,max=200, step=1, value=8), b=widgets.FloatSlider(min=-100,max=100, step=1, value=-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f00313e",
   "metadata": {},
   "source": [
    "In batch processing, he result of a batch of data processed by a linear layer, i.e. $\\mathbf{y} = \\mathbf{Wx}$, is processed by the non-linearity in an element-wise fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46580715",
   "metadata": {},
   "source": [
    "## Universal approximation theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2f8240",
   "metadata": {},
   "source": [
    "It could be shown, that a neural network consisting only of one (sufficiently large) linear layer and one non-linear layer which are combined by a single linear building block can approximate any continuous function. \n",
    "\n",
    "TODO: Add figure.\n",
    "\n",
    "In the following, we will sketch the proof of that theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057a2e73",
   "metadata": {},
   "source": [
    "Consider again the sigmoid function $\\psi$ from before applied to a linear block, i.e., $\\psi(wx+b)$  form before. \n",
    "\n",
    "The position of the unit step approximated by $\\psi$ for high $w$ resides at the position $s=-\\frac{b}{w}$. Since this is easier to interpret, we will focus on the parameter $s$ from now on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f378eb",
   "metadata": {},
   "source": [
    "We now consider two of such blocks added together by an additional single linear building block, i.e.,\n",
    "\n",
    "$\\begin{align} \n",
    "   \\psi_{s_1} (x)\\cdot w_1 + \\psi_{s_2}(x)\\cdot w_2  + b\n",
    "\\end{align}$\n",
    "\n",
    "with the respective positions $s_1, s_2$ of the step functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfffa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, s):\n",
    "    w = 1000\n",
    "    b = -1 * w * s\n",
    "    return x*w + b\n",
    "def plot_2_neurons(s1, s2, w1, w2):\n",
    "    xs = np.linspace(0,1,1000)\n",
    "    y1s = sigmoid(linear(xs, s1))\n",
    "    y2s = sigmoid(linear(xs, s2))\n",
    "    res = w1 * y1s + w2 * y2s\n",
    "    plt.clf()\n",
    "    plt.plot(xs,res)\n",
    "plt.figure()\n",
    "interact(lambda s1, s2, w1, w2: plot_2_neurons(s1, s2, w1, w2), \\\n",
    "         s1 = widgets.FloatSlider(min=0.0,max=1.0, step=0.1, value=0.2), \\\n",
    "         s2 = widgets.FloatSlider(min=0.0,max=1.0, step=0.1, value=0.6), \\\n",
    "         w1 = widgets.FloatSlider(min=-2,max=2, step=0.1, value=0.4), \\\n",
    "         w2 = widgets.FloatSlider(min=-2,max=2, step=0.1, value=0.6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee1ea2f",
   "metadata": {},
   "source": [
    "As can be seen, this addition yields to consecutive step functions what can be used, e.g., to approximate the $\\mathrm{rect}$-function.\n",
    "\n",
    "Therefore, if $s_1 < s_2$, it must hold $w_2 = -w_1$ to get a $\\mathrm{rect}$-function with height $h=\\left| w_1 \\right| = \\left| w_2 \\right|  $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9843c",
   "metadata": {},
   "source": [
    "We can now add two of such pairs of blocks together to model two $\\mathrm{rect}$-functions, i.e., with start, stop positions $s_{1,1}, s_{1,2}$ and height $h_1$ of the first $\\mathrm{rect}$-function and $s_{2,1}, s_{2,2}, h_2$ for the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d2f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_approx(x, s1, s2, h):\n",
    "    w1 = h\n",
    "    w2 = -1 * h\n",
    "    y1s = sigmoid(linear(x, s1))\n",
    "    y2s = sigmoid(linear(x, s2))\n",
    "    return w1 * y1s + w2 * y2s\n",
    "    \n",
    "def plot_2_rects(s11, s12, s21, s22, h1, h2):\n",
    "    xs = np.linspace(0,1,1000)\n",
    "    y1s = rect_approx(xs, s11, s12, h1)\n",
    "    y2s = rect_approx(xs, s21, s22, h2)\n",
    "    res = y1s + y2s\n",
    "    plt.clf()\n",
    "    plt.plot(xs,res)\n",
    "plt.figure()\n",
    "interact(lambda s11, s12, s21, s22, h1, h2: plot_2_rects(s11, s12, s21, s22, h1, h2), \\\n",
    "         s11 = widgets.FloatSlider(min=0.0,max=1.0, step=0.1, value=0.2), \\\n",
    "         s12 = widgets.FloatSlider(min=0.0,max=1.0, step=0.1, value=0.3), \\\n",
    "         s21 = widgets.FloatSlider(min=0.0,max=1.0, step=0.1, value=0.4), \\\n",
    "         s22 = widgets.FloatSlider(min=0.0,max=1.0, step=0.1, value=0.6), \\\n",
    "         h1 = widgets.FloatSlider(min=-2,max=2, step=0.1, value=0.3), \\\n",
    "         h2 = widgets.FloatSlider(min=-2,max=2, step=0.1, value=-0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4d402",
   "metadata": {},
   "source": [
    "The more of these modules we add, the more complicated the shape of the output can be. When the width of the single $\\mathrm{rect}$-functions approaches zero and the number of $\\mathrm{rect}$-functions approaches infinity, any continuous function can be approximated.\n",
    "\n",
    "This also holds for higher dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab114d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-0.2,1.2,1000)\n",
    "s1s = np.array([0, 0.2, 0.4, 0.6, 0.8])\n",
    "s2s = np.array([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "hs = np.array([0.1, 0.7, 0.1, -0.1, -0.8])\n",
    "res = np.zeros_like(xs)\n",
    "\n",
    "for i in range(0, s1s.shape[0]):\n",
    "    res = res + rect_approx(xs, s1s[i], s2s[i], hs[i])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xs,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebf7195",
   "metadata": {},
   "source": [
    "Although the theorem states that such a simple structure is enough to approximate any function, in practice network structures with more but thinner layers (i.e., with fewer building blocks) are employed as they can achieve similar results with notably fewer building blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4579075",
   "metadata": {},
   "source": [
    "## Optimization of network parameters\n",
    "\n",
    "How can we automatically determine the parameters $\\boldsymbol{\\theta}$ of the network so that $\\forall (\\mathbf{x},\\mathbf{y}) \\in \\mathcal{T}:\\text{dist}(\\varphi_\\boldsymbol{\\theta}(\\mathbf{x}), \\mathbf{y} ) \\rightarrow \\text{Min.}\\,$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b197cd8",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7088a42d",
   "metadata": {},
   "source": [
    "To find this minimum, we compute the gradient $\\nabla\\text{dist}_\\boldsymbol{\\theta}$ of the distance function $\\text{dist}$ with respect to the network parameters $\\boldsymbol{\\theta}$.\n",
    "\n",
    "We can then iteratively update an initial guess $\\hat{\\boldsymbol{\\theta}}_0$ (e.g., random) of the network parameters by pushing it into the inverse direction of the gradient $\\nabla\\text{dist}_\\boldsymbol{\\theta}$, i.e., into the direction of the nearest minimum:\n",
    "\n",
    "$\\begin{align} \n",
    "   \\hat{\\boldsymbol{\\theta}}_{i+1} \\leftarrow \\hat{\\boldsymbol{\\theta}}_{i} - \\eta \\nabla\\text{dist}_\\boldsymbol{\\theta}(\\varphi_{\\hat{\\boldsymbol{\\theta}_i}}(\\mathbf{X}), \\mathbf{Y} )\\,, (\\mathbf{X},\\mathbf{Y}) \\in \\mathcal{T}\n",
    "\\end{align}$\n",
    "\n",
    "with $\\mathbf{X}, \\mathbf{Y}$ indicating batches of multiple training vectors $(\\mathbf{x}, \\mathbf{y}) \\in \\mathcal{T}$ and $\\eta$ denoting the so-called *learning rate* or the *step size* for the gradient descent updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5302d",
   "metadata": {},
   "source": [
    "* In every gradient descent iteration, the parameters are updated with regard to the respective batch of training samples $(\\mathbf{X}, \\mathbf{Y})$ chosen in that iteration.\n",
    "* In practical scenarios it is usually not possible to process the whole training set in one gradient descent step as the respective data would not fit into the available memory. \n",
    "* Hence, in every iteration another batch of training data is used, so that eventually all training samples have been used. The partitioning of $\\mathcal{T}$ into those batches is usually performed randomly. This is why this kind of gradient descent is sometimes referred to as *stochastic gradient descent*.\n",
    "* The set of gradient descent iterations needed to cycle through all training data once is a so-called *epoch*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb4c90",
   "metadata": {},
   "source": [
    "### The chain rule and the backpropagation algorithm\n",
    "\n",
    "Calculating the gradient required for gradient descent by hand can be infeasible for large expressions. Fortunately, this process can be automated as we will see in the following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fb0d5",
   "metadata": {},
   "source": [
    "We start with an example.\n",
    "\n",
    "Assume we want to calculate the derivative (i.e., the gradient) of $y = \\log (x)^2$ with respect to $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a07365",
   "metadata": {},
   "source": [
    "We can express this function via intermediate variables for all basic math operations:\n",
    "\n",
    "$\\begin{align} \n",
    "   y &= v_2 \\\\\n",
    "   v_2 &= v_1^2 \\\\\n",
    "   v_1 &= \\log (x) \\,.\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05caaa",
   "metadata": {},
   "source": [
    "In order to find the derivate of $y$ with respect to $x$, we can iteratively apply the chain rule to obtain:\n",
    "\n",
    "$\\begin{align} \n",
    "   \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial v_2} \\frac{\\partial v_2}{\\partial x} = \\frac{\\partial y}{\\partial v_2} \\frac{\\partial v_2}{\\partial v_1} \\frac{\\partial v_1}{\\partial x} \\,.\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33743d9b",
   "metadata": {},
   "source": [
    "Those partial derivatives can easily be obtained for the respective terms:\n",
    "\n",
    "$\\begin{align} \n",
    "  \\frac{\\partial y}{\\partial v_2} &= 1 \\\\\n",
    "  \\frac{\\partial v_2}{\\partial v_1} &= 2v_1 \\\\\n",
    "  \\frac{\\partial v_1}{\\partial x} &= \\frac{1}{x}.\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a10a114",
   "metadata": {},
   "source": [
    "Hence, the sought derivate of $y$ w.r.t. $x$ is given by:\n",
    "\n",
    "$\\begin{align} \n",
    "  \\frac{\\partial y}{\\partial x} = \\frac{\\partial y}{\\partial v_2} \\frac{\\partial v_2}{\\partial v_1} \\frac{\\partial v_1}{\\partial x} = 1\\cdot 2v_1 \\cdot \\frac{1}{x} \\,.\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c174b8",
   "metadata": {},
   "source": [
    "This expression can be evaluated in reverse order (the so-called *backward pass*), i.e., from left to right, when the intermediate variables $v_2, v_1$ have been evaluated for some $x$ in the so-called *forward pass*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4c9db0",
   "metadata": {},
   "source": [
    "When every basic mathematical function (e.g., $+, -, \\exp, \\log, \\sin, \\ldots$) also provides a method for the calculation of its gradient, the process of obtaining the derivative of an arbitrary complex expression, with respect to some variable and given the actual input numbers for the parameter(s), can be automated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9965a9c",
   "metadata": {},
   "source": [
    "During the forward pass, a calculation graph is constructed that represents the hierarchical relations of the individual mathematical functions in terms of the chain rule. The intermediate results for every operation are saved in the nodes of this graph. Then, the graph can be traversed backwards starting from all leaves and intermediate gradient values can be calculated for every node and can then be backpropagated to the root yielding the sought gradient.\n",
    "\n",
    "This approach is also known as the *backpropagation algorithm*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c352f",
   "metadata": {},
   "source": [
    "##### Example implementation for $y = \\log (x)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56984221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(inp):\n",
    "    return inp**2\n",
    "\n",
    "def ln(inp):\n",
    "    return np.log(inp)\n",
    "\n",
    "def square_grad(inp, out):\n",
    "    inp.g = 2*inp*out.g\n",
    "    \n",
    "def ln_grad(inp, out):\n",
    "    inp.g = 1/inp * out.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b5fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def y(x):\n",
    "    x = torch.tensor(x)\n",
    "    v1 = ln(x)\n",
    "    v2 = square(r1)\n",
    "        \n",
    "    v2.g = 1\n",
    "    square_grad(v1, v2)\n",
    "    ln_grad(x, v1)\n",
    "    \n",
    "    return v2, x.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c75d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2917aa",
   "metadata": {},
   "source": [
    "Note: We just used PyTorch here so that we can easily introduce the property `.g` for all our variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c2726e",
   "metadata": {},
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "There are various libraries that provide the functionality of backpropagation mentioned above without the user (i.e., the programmer) having to explicitly control it. Often a technique called *operator overloading* is employed that builds the computation graph required for the gradient calculation behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060653e9",
   "metadata": {},
   "source": [
    "In PyTorch, calculating the gradient of our equation $y = \\log (x)^2$ w.r.t. $x$ could be achieved like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4dafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of our function using torch-routines:\n",
    "def y_torch(x):\n",
    "    return torch.square(torch.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e246874",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(3.0)    # Our input variable\n",
    "x.requires_grad = True   # We need the gradient with respect to x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1867bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = y_torch(x)      # Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e11acc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.backward()        # Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e2b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad                   # Access the gradient"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.14.1"
   }
  },
  "kernelspec": {
   "display_name": "compimg",
   "language": "python",
   "name": "compimg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "source_map": [
   15,
   34,
   56,
   63,
   75,
   87,
   101,
   116,
   120,
   124,
   138,
   142,
   154,
   162,
   168,
   173,
   177,
   181,
   191,
   197,
   201,
   213,
   241,
   269,
   273,
   281,
   285,
   296,
   299,
   303,
   307,
   315,
   321,
   331,
   349,
   355,
   359,
   382,
   388,
   400,
   404,
   410,
   414,
   426,
   433,
   439,
   445,
   455,
   463,
   473,
   481,
   485,
   489,
   495,
   499,
   513,
   526,
   528,
   532,
   538,
   542,
   548,
   553,
   557,
   561
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}